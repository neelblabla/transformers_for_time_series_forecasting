{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ed103c66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed103c66",
        "outputId": "23720654-ce4e-47e4-a19b-491b6bc4f8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "# !pip install tsai\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "5522d111",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "5522d111",
        "outputId": "34ca6103-781a-4770-db5c-cbabf25ecb22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Date Time  p (mbar)  T (degC)  rh (%)  sh (g/kg)  Tpot (K)  \\\n",
              "0      01.01.2020 00:10:04   1019.28     -0.02    96.7       3.61    271.66   \n",
              "1      01.01.2020 00:20:04   1019.19      0.04    96.6       3.62    271.72   \n",
              "2      01.01.2020 00:30:04   1019.11      0.10    97.2       3.66    271.78   \n",
              "3      01.01.2020 00:40:04   1019.04      0.13    97.7       3.68    271.82   \n",
              "4      01.01.2020 00:50:04   1018.98      0.00    97.9       3.66    271.69   \n",
              "...                    ...       ...       ...     ...        ...       ...   \n",
              "52529  31.12.2020 23:20:00    988.15      2.16    85.8       3.86    276.25   \n",
              "52530  31.12.2020 23:30:00    988.13      2.15    88.6       3.99    276.24   \n",
              "52531  31.12.2020 23:40:00    988.07      2.03    88.8       3.96    276.13   \n",
              "52532  31.12.2020 23:50:00    988.08      1.93    88.5       3.92    276.03   \n",
              "52533  01.01.2021 00:00:00    988.07      1.87    89.9       3.97    275.97   \n",
              "\n",
              "       Tdew (degC)  VPmax (mbar)  VPact (mbar)  VPdef (mbar)  ...  \\\n",
              "0            -0.48          6.10          5.90          0.20  ...   \n",
              "1            -0.43          6.13          5.92          0.21  ...   \n",
              "2            -0.29          6.15          5.98          0.17  ...   \n",
              "3            -0.19          6.16          6.02          0.14  ...   \n",
              "4            -0.29          6.11          5.98          0.13  ...   \n",
              "...            ...           ...           ...           ...  ...   \n",
              "52529         0.04          7.14          6.13          1.01  ...   \n",
              "52530         0.47          7.13          6.32          0.81  ...   \n",
              "52531         0.39          7.07          6.28          0.79  ...   \n",
              "52532         0.24          7.02          6.22          0.81  ...   \n",
              "52533         0.40          6.99          6.29          0.71  ...   \n",
              "\n",
              "       ST008 (degC)  ST016 (degC)  ST032 (degC)  ST064 (degC)  ST128 (degC)  \\\n",
              "0              2.08          2.88          3.90          6.14          8.81   \n",
              "1              2.07          2.88          3.90          6.14          8.80   \n",
              "2              2.06          2.87          3.90          6.14          8.81   \n",
              "3              2.06          2.87          3.90          6.13          8.81   \n",
              "4              2.05          2.87          3.90          6.13          8.80   \n",
              "...             ...           ...           ...           ...           ...   \n",
              "52529          2.65          3.24          4.16          6.09          8.67   \n",
              "52530          2.66          3.24          4.16          6.09          8.67   \n",
              "52531          2.66          3.24          4.16          6.09          8.67   \n",
              "52532          2.67          3.25          4.16          6.09          8.67   \n",
              "52533          2.67          3.25          4.16          6.09          8.67   \n",
              "\n",
              "       SM008 (%)  SM016 (%)  SM032 (%)  SM064 (%)  SM128 (%)  \n",
              "0          32.50      30.09      31.64      21.55      29.76  \n",
              "1          32.50      30.09      31.64      21.55      29.76  \n",
              "2          32.50      30.09      31.64      21.55      29.76  \n",
              "3          32.50      30.09      31.64      21.55      29.76  \n",
              "4          32.50      30.09      31.64      21.55      29.76  \n",
              "...          ...        ...        ...        ...        ...  \n",
              "52529      37.31      33.10      33.01      30.75      31.80  \n",
              "52530      37.32      33.10      33.01      30.75      31.80  \n",
              "52531      37.32      33.10      33.01      30.75      31.80  \n",
              "52532      37.32      33.09      33.01      30.76      31.80  \n",
              "52533      37.32      33.09      33.01      30.76      31.80  \n",
              "\n",
              "[52534 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22b7d7aa-8330-4b1c-8a97-9eb2b46db77d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date Time</th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>Tpot (K)</th>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>...</th>\n",
              "      <th>ST008 (degC)</th>\n",
              "      <th>ST016 (degC)</th>\n",
              "      <th>ST032 (degC)</th>\n",
              "      <th>ST064 (degC)</th>\n",
              "      <th>ST128 (degC)</th>\n",
              "      <th>SM008 (%)</th>\n",
              "      <th>SM016 (%)</th>\n",
              "      <th>SM032 (%)</th>\n",
              "      <th>SM064 (%)</th>\n",
              "      <th>SM128 (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01.01.2020 00:10:04</td>\n",
              "      <td>1019.28</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>96.7</td>\n",
              "      <td>3.61</td>\n",
              "      <td>271.66</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>6.10</td>\n",
              "      <td>5.90</td>\n",
              "      <td>0.20</td>\n",
              "      <td>...</td>\n",
              "      <td>2.08</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.14</td>\n",
              "      <td>8.81</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01.01.2020 00:20:04</td>\n",
              "      <td>1019.19</td>\n",
              "      <td>0.04</td>\n",
              "      <td>96.6</td>\n",
              "      <td>3.62</td>\n",
              "      <td>271.72</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>6.13</td>\n",
              "      <td>5.92</td>\n",
              "      <td>0.21</td>\n",
              "      <td>...</td>\n",
              "      <td>2.07</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.14</td>\n",
              "      <td>8.80</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01.01.2020 00:30:04</td>\n",
              "      <td>1019.11</td>\n",
              "      <td>0.10</td>\n",
              "      <td>97.2</td>\n",
              "      <td>3.66</td>\n",
              "      <td>271.78</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>6.15</td>\n",
              "      <td>5.98</td>\n",
              "      <td>0.17</td>\n",
              "      <td>...</td>\n",
              "      <td>2.06</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.14</td>\n",
              "      <td>8.81</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01.01.2020 00:40:04</td>\n",
              "      <td>1019.04</td>\n",
              "      <td>0.13</td>\n",
              "      <td>97.7</td>\n",
              "      <td>3.68</td>\n",
              "      <td>271.82</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>6.16</td>\n",
              "      <td>6.02</td>\n",
              "      <td>0.14</td>\n",
              "      <td>...</td>\n",
              "      <td>2.06</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.13</td>\n",
              "      <td>8.81</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01.01.2020 00:50:04</td>\n",
              "      <td>1018.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>97.9</td>\n",
              "      <td>3.66</td>\n",
              "      <td>271.69</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>6.11</td>\n",
              "      <td>5.98</td>\n",
              "      <td>0.13</td>\n",
              "      <td>...</td>\n",
              "      <td>2.05</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.13</td>\n",
              "      <td>8.80</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52529</th>\n",
              "      <td>31.12.2020 23:20:00</td>\n",
              "      <td>988.15</td>\n",
              "      <td>2.16</td>\n",
              "      <td>85.8</td>\n",
              "      <td>3.86</td>\n",
              "      <td>276.25</td>\n",
              "      <td>0.04</td>\n",
              "      <td>7.14</td>\n",
              "      <td>6.13</td>\n",
              "      <td>1.01</td>\n",
              "      <td>...</td>\n",
              "      <td>2.65</td>\n",
              "      <td>3.24</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.31</td>\n",
              "      <td>33.10</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.75</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52530</th>\n",
              "      <td>31.12.2020 23:30:00</td>\n",
              "      <td>988.13</td>\n",
              "      <td>2.15</td>\n",
              "      <td>88.6</td>\n",
              "      <td>3.99</td>\n",
              "      <td>276.24</td>\n",
              "      <td>0.47</td>\n",
              "      <td>7.13</td>\n",
              "      <td>6.32</td>\n",
              "      <td>0.81</td>\n",
              "      <td>...</td>\n",
              "      <td>2.66</td>\n",
              "      <td>3.24</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.32</td>\n",
              "      <td>33.10</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.75</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52531</th>\n",
              "      <td>31.12.2020 23:40:00</td>\n",
              "      <td>988.07</td>\n",
              "      <td>2.03</td>\n",
              "      <td>88.8</td>\n",
              "      <td>3.96</td>\n",
              "      <td>276.13</td>\n",
              "      <td>0.39</td>\n",
              "      <td>7.07</td>\n",
              "      <td>6.28</td>\n",
              "      <td>0.79</td>\n",
              "      <td>...</td>\n",
              "      <td>2.66</td>\n",
              "      <td>3.24</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.32</td>\n",
              "      <td>33.10</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.75</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52532</th>\n",
              "      <td>31.12.2020 23:50:00</td>\n",
              "      <td>988.08</td>\n",
              "      <td>1.93</td>\n",
              "      <td>88.5</td>\n",
              "      <td>3.92</td>\n",
              "      <td>276.03</td>\n",
              "      <td>0.24</td>\n",
              "      <td>7.02</td>\n",
              "      <td>6.22</td>\n",
              "      <td>0.81</td>\n",
              "      <td>...</td>\n",
              "      <td>2.67</td>\n",
              "      <td>3.25</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.32</td>\n",
              "      <td>33.09</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.76</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52533</th>\n",
              "      <td>01.01.2021 00:00:00</td>\n",
              "      <td>988.07</td>\n",
              "      <td>1.87</td>\n",
              "      <td>89.9</td>\n",
              "      <td>3.97</td>\n",
              "      <td>275.97</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.99</td>\n",
              "      <td>6.29</td>\n",
              "      <td>0.71</td>\n",
              "      <td>...</td>\n",
              "      <td>2.67</td>\n",
              "      <td>3.25</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.32</td>\n",
              "      <td>33.09</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.76</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52534 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22b7d7aa-8330-4b1c-8a97-9eb2b46db77d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-22b7d7aa-8330-4b1c-8a97-9eb2b46db77d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-22b7d7aa-8330-4b1c-8a97-9eb2b46db77d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b3a7a99e-f061-4397-92c5-c61400006420\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b3a7a99e-f061-4397-92c5-c61400006420')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b3a7a99e-f061-4397-92c5-c61400006420 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/')\n",
        "\n",
        "\n",
        "df=pd.read_csv('weather.csv')\n",
        "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "bffcb3d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bffcb3d9",
        "outputId": "06a2a200-d382-4434-b758-2420f9451964"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndf=pd.read_csv('weather.csv',index_col=None)\\ndf.drop('Unnamed: 0',axis=1,inplace=True)\\ndf\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "'''\n",
        "df=pd.read_csv('weather.csv',index_col=None)\n",
        "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "df\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "6JcUYXLa_2It",
      "metadata": {
        "id": "6JcUYXLa_2It"
      },
      "outputs": [],
      "source": [
        "# df=df.iloc[:,0:9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "IMhozIdUX4fg",
      "metadata": {
        "id": "IMhozIdUX4fg"
      },
      "outputs": [],
      "source": [
        "class weather_data(torch.utils.data.Dataset):\n",
        "    def __init__(self, df,scale=True, mode=\"train\", seq_len=336, pred_len=96,num_feat=8):\n",
        "        super().__init__()\n",
        "        self.num_feat=num_feat\n",
        "        self.df = df.iloc[:,1:num_feat+1]\n",
        "        # time_stamp = df.iloc[:,0]\n",
        "\n",
        "        assert mode in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[mode]\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        dataset_len=len(df)\n",
        "\n",
        "        border1s = [0, int(round(0.8*dataset_len,0)) - self.seq_len, int(round(0.8*dataset_len,0)) + int(round(0.1*dataset_len,0)) - self.seq_len]\n",
        "        border2s = [int(round(0.8*dataset_len,0)), int(round(0.8*dataset_len,0)) + int(round(0.1*dataset_len,0)), int(round(0.8*dataset_len,0)) + int(round(0.1*dataset_len,0)) + int(round(0.1*dataset_len,0))]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "\n",
        "        if scale:\n",
        "          scaler = StandardScaler()\n",
        "          self.df = self.df.to_numpy(dtype=np.float32)\n",
        "          self.df = scaler.fit_transform(self.df)\n",
        "\n",
        "        else:\n",
        "          self.df = self.df.to_numpy(dtype=np.float32)\n",
        "        # time_stamp = time_stamp.to_numpy()\n",
        "\n",
        "        self.data_x = self.df[border1: border2, :]\n",
        "        self.data_y = self.df[border1: border2, -1]\n",
        "\n",
        "        # self.data_stamp = time_stamp[border1: border2]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end\n",
        "        r_end = r_begin + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        return seq_x, seq_y\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "Ds8C5Wv1TZJy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds8C5Wv1TZJy",
        "outputId": "d9c2f134-409f-4aa2-a02e-7c89a3ffc3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41596 5158 5158\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = weather_data(df=df)\n",
        "valid_dataset = weather_data(df=df,mode=\"val\")\n",
        "test_dataset = weather_data(df=df,mode=\"test\")\n",
        "\n",
        "print(len(train_dataset),len(valid_dataset),len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "70a18f29",
      "metadata": {
        "id": "70a18f29"
      },
      "outputs": [],
      "source": [
        "#Implementation of head layer\n",
        "\n",
        "class Flatten_Head(torch.nn.Module):\n",
        "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
        "        super().__init__()\n",
        "        self.n_vars = n_vars\n",
        "        self.flatten = torch.nn.Flatten(start_dim=-3)\n",
        "        self.linear = torch.nn.Linear(nf * n_vars, target_window)\n",
        "        self.dropout = torch.nn.Dropout(head_dropout)\n",
        "\n",
        "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
        "        x = self.flatten(x)                               # x: [bs x nvars * d_model * patch_num]\n",
        "        x = self.linear(x)                                # x: [bs x target_window]\n",
        "        x = self.dropout(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "28abbfa0",
      "metadata": {
        "id": "28abbfa0"
      },
      "outputs": [],
      "source": [
        "#implementation of utils\n",
        "class RevIN(torch.nn.Module):\n",
        "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False, target_idx=-1):\n",
        "        super(RevIN, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.subtract_last = subtract_last\n",
        "        self.target_idx = target_idx\n",
        "        if self.affine:\n",
        "            self._init_params()\n",
        "\n",
        "    def forward(self, x, mode):\n",
        "        if mode == \"norm\":\n",
        "            self._get_statistics(x)\n",
        "            x = self._normalize(x)\n",
        "        elif mode == \"denorm\":\n",
        "            x = self._denormalize(x)\n",
        "        else: raise AssertionError\n",
        "        return x\n",
        "\n",
        "    def _init_params(self):\n",
        "        self.affine_weight = torch.nn.Parameter(torch.ones(self.num_features))\n",
        "        self.affine_bias = torch.nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "    def _get_statistics(self, x):\n",
        "        dim2reduce = tuple(range(1, x.ndim-1))\n",
        "        if self.subtract_last:\n",
        "            self.last = x[:,-1,:].unsqueeze(1)\n",
        "        else:\n",
        "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "    def _normalize(self, x):\n",
        "        if self.subtract_last:\n",
        "            x = x - self.last\n",
        "        else:\n",
        "            x = x - self.mean\n",
        "        x = x / self.stdev\n",
        "        if self.affine:\n",
        "            x = x * self.affine_weight\n",
        "            x = x + self.affine_bias\n",
        "        return x\n",
        "\n",
        "    def _denormalize(self, x):\n",
        "        if self.affine:\n",
        "            x = x - self.affine_bias\n",
        "            x = x / (self.affine_weight + self.eps*self.eps)\n",
        "        x = x * self.stdev[:, :, self.target_idx]\n",
        "        if self.subtract_last:\n",
        "            x = x + self.last[:, :, self.target_idx]\n",
        "        else:\n",
        "            x = x + self.mean[:, :, self.target_idx]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transpose(torch.nn.Module):\n",
        "    def __init__(self, *dims, contiguous=False):\n",
        "        super().__init__()\n",
        "        self.dims, self.contiguous = dims, contiguous\n",
        "    def forward(self, x):\n",
        "        if self.contiguous:\n",
        "            return x.transpose(*self.dims).contiguous()\n",
        "        else:\n",
        "            return x.transpose(*self.dims)\n",
        "\n",
        "\n",
        "def positional_encoding(q_len, d_model):\n",
        "    W_pos = torch.empty((q_len, d_model))\n",
        "    torch.nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    return torch.nn.Parameter(W_pos, requires_grad=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e1f414ee",
      "metadata": {
        "id": "e1f414ee"
      },
      "outputs": [],
      "source": [
        "#Implementation of PatchTST Encorder layer\n",
        "class TSTiEncoder(torch.nn.Module):  #i means channel-independent\n",
        "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
        "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
        "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., store_attn=False,\n",
        "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
        "                 verbose=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.patch_num = patch_num\n",
        "        self.patch_len = patch_len\n",
        "        # Input encoding\n",
        "        q_len = patch_num\n",
        "        self.W_P = torch.nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
        "        self.seq_len = q_len\n",
        "        # Positional encoding\n",
        "        self.W_pos = positional_encoding(q_len, d_model)\n",
        "        # Residual dropout\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        # Encoder\n",
        "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                   pre_norm=pre_norm, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
        "\n",
        "    def forward(self, x):                                              # x: [bs x nvars x patch_len x patch_num]\n",
        "\n",
        "        n_vars = x.shape[1]\n",
        "        # Input encoding\n",
        "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
        "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
        "\n",
        "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
        "        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
        "\n",
        "        # Encoder\n",
        "        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
        "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
        "        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "# Cell\n",
        "class TSTEncoder(torch.nn.Module):\n",
        "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None,\n",
        "                        norm='BatchNorm', attn_dropout=0., dropout=0.,\n",
        "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
        "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                                      res_attention=res_attention,\n",
        "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
        "        self.res_attention = res_attention\n",
        "\n",
        "    def forward(self, src, key_padding_mask=None, attn_mask=None):\n",
        "        output = src\n",
        "        scores = None\n",
        "        if self.res_attention:\n",
        "            for mod in self.layers:\n",
        "                output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n",
        "        else:\n",
        "            for mod in self.layers:\n",
        "                output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "class TSTEncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
        "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, res_attention=False, pre_norm=False):\n",
        "        super().__init__()\n",
        "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        # Multi-Head attention\n",
        "        self.res_attention = res_attention\n",
        "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_attn = torch.nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_attn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_attn = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "        # Position-wise Feed-Forward\n",
        "        self.ff = torch.nn.Sequential(torch.nn.Linear(d_model, d_ff, bias=bias),\n",
        "                                torch.nn.GELU(),\n",
        "                                torch.nn.Dropout(dropout),\n",
        "                                torch.nn.Linear(d_ff, d_model, bias=bias))\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_ffn = torch.nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_ffn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_ffn = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.store_attn = store_attn\n",
        "\n",
        "\n",
        "    def forward(self, src, prev=None, key_padding_mask=None, attn_mask=None):\n",
        "\n",
        "        # Multi-Head attention sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "        ## Multi-Head attention\n",
        "        if self.res_attention:\n",
        "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        if self.store_attn:\n",
        "            self.attn = attn\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "        ## Position-wise Feed-Forward\n",
        "        src2 = self.ff(src)\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "\n",
        "        if self.res_attention:\n",
        "            return src, scores\n",
        "        else:\n",
        "            return src\n",
        "\n",
        "\n",
        "class _MultiheadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
        "        super().__init__()\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
        "\n",
        "        self.W_Q = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_K = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_V = torch.nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
        "\n",
        "        # Scaled Dot-Product Attention (multiple heads)\n",
        "        self.res_attention = res_attention\n",
        "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
        "\n",
        "        # Poject output\n",
        "        self.to_out = torch.nn.Sequential(torch.nn.Linear(n_heads * d_v, d_model), torch.nn.Dropout(proj_dropout))\n",
        "\n",
        "    def forward(self, Q, K=None, V=None, prev=None,\n",
        "                key_padding_mask=None, attn_mask=None):\n",
        "\n",
        "        bs = Q.size(0)\n",
        "        if K is None: K = Q\n",
        "        if V is None: V = Q\n",
        "\n",
        "        # Linear (+ split in multiple heads)\n",
        "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
        "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
        "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
        "\n",
        "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
        "        if self.res_attention:\n",
        "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # back to the original inputs dimensions\n",
        "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
        "        output = self.to_out(output)\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights\n",
        "\n",
        "\n",
        "class _ScaledDotProductAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
        "        super().__init__()\n",
        "        self.attn_dropout = torch.nn.Dropout(attn_dropout)\n",
        "        self.res_attention = res_attention\n",
        "        head_dim = d_model // n_heads\n",
        "        self.scale = torch.nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
        "        self.lsa = lsa\n",
        "\n",
        "    def forward(self, q, k, v, prev=None, key_padding_mask=None, attn_mask=None):\n",
        "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
        "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # Add pre-softmax attention scores from the previous layer (optional)\n",
        "        if prev is not None: attn_scores = attn_scores + prev\n",
        "\n",
        "        # Attention mask (optional)\n",
        "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
        "            if attn_mask.dtype == torch.bool:\n",
        "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
        "            else:\n",
        "                attn_scores += attn_mask\n",
        "\n",
        "        # Key padding mask (optional)\n",
        "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
        "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
        "\n",
        "        # normalize the attention weights\n",
        "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # compute the new values given the attention weights\n",
        "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "02761932",
      "metadata": {
        "id": "02761932"
      },
      "outputs": [],
      "source": [
        "#Implementation of PatchTST\n",
        "\n",
        "class PatchTST(torch.nn.Module):\n",
        "    def __init__(self, c_in, context_window, target_window, patch_len, stride, max_seq_len=1024,\n",
        "                 n_layers=3, d_model=16, n_heads=4, d_k=None, d_v=None,\n",
        "                 d_ff=128, attn_dropout=0.0, dropout=0.3, key_padding_mask='auto',\n",
        "                 padding_var=None, attn_mask=None, res_attention=True, pre_norm=False, store_attn=False,\n",
        "                 head_dropout = 0.0, padding_patch = \"end\",\n",
        "                 revin = True, affine = False, subtract_last = False,\n",
        "                 verbose=False, target_idx=-1, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.revin = revin\n",
        "        if revin:\n",
        "            self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last, target_idx=target_idx)\n",
        "\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding_patch = padding_patch\n",
        "        patch_num = int((context_window - patch_len)/stride + 1)\n",
        "\n",
        "        if padding_patch == \"end\":\n",
        "            self.padding_patch_layer = torch.nn.ReplicationPad1d((0, stride))\n",
        "            patch_num += 1\n",
        "\n",
        "        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
        "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
        "                                attn_dropout=attn_dropout, dropout=dropout, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
        "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
        "                                verbose=verbose, **kwargs)\n",
        "\n",
        "        self.head_nf = d_model * patch_num\n",
        "        self.n_vars = c_in\n",
        "\n",
        "        self.head = Flatten_Head(self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
        "\n",
        "    def forward(self, z):                                                                   # z: [bs x seq_len × nvars]\n",
        "        # instance norm\n",
        "        if self.revin:\n",
        "            z = self.revin_layer(z, 'norm')\n",
        "            z = z.permute(0,2,1)                                                            # z: [bs x nvars × seq_len]\n",
        "\n",
        "        # do patching\n",
        "        if self.padding_patch == 'end':\n",
        "            z = self.padding_patch_layer(z)\n",
        "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
        "        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
        "\n",
        "        # model\n",
        "        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
        "        z = self.head(z)                                                                    # z: [bs x target_window]\n",
        "\n",
        "        # denorm\n",
        "        if self.revin:\n",
        "            z = self.revin_layer(z, 'denorm')\n",
        "        return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "b7f6dca5",
      "metadata": {
        "id": "b7f6dca5"
      },
      "outputs": [],
      "source": [
        "#Linear model\n",
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, c_in, context_window, target_window):\n",
        "        super().__init__()\n",
        "        self.c_in = c_in\n",
        "        self.context_winsoq = context_window\n",
        "        self.target_window = target_window\n",
        "\n",
        "        self.flatten = torch.nn.Flatten(start_dim=-2)\n",
        "\n",
        "        self.linear = torch.nn.Linear(c_in * context_window, target_window)\n",
        "\n",
        "    def forward(self, x):                   # x: [bs x seq_len × nvars]\n",
        "        x = self.flatten(x)                 # x: [bs x seq_len * nvars]\n",
        "        x = self.linear(x)                  # x: [bs x target_window]\n",
        "        return x\n",
        "\n",
        "\n",
        "class moving_avg(torch.nn.Module):\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # padding on the both ends of time series\n",
        "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        x = torch.cat([front, x, end], dim=1)\n",
        "        x = self.avg(x.permute(0, 2, 1))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class series_decomp(torch.nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean\n",
        "\n",
        "class DLinear(torch.nn.Module):\n",
        "    def __init__(self, c_in, context_window, target_window):\n",
        "        super().__init__()\n",
        "        # Decompsition Kernel Size\n",
        "        kernel_size = 25\n",
        "        self.decompsition = series_decomp(kernel_size)\n",
        "        self.flatten_Seasonal = torch.nn.Flatten(start_dim=-2)\n",
        "        self.flatten_Trend = torch.nn.Flatten(start_dim=-2)\n",
        "\n",
        "        self.Linear_Seasonal = torch.nn.Linear(c_in * context_window, target_window)\n",
        "        self.Linear_Trend = torch.nn.Linear(c_in * context_window, target_window)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Input length, Channel]\n",
        "        seasonal_init, trend_init = self.decompsition(x)\n",
        "        seasonal_init = self.flatten_Seasonal(x)\n",
        "        trend_init = self.flatten_Trend(x)\n",
        "\n",
        "        seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
        "        trend_output = self.Linear_Trend(trend_init)\n",
        "\n",
        "        x = seasonal_output + trend_output\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "032b97a3",
      "metadata": {
        "id": "032b97a3"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class model_run:\n",
        "    def __init__(self,model=PatchTST,c_in = train_dataset.num_feat,context_window = 336,target_window = 96,patch_len = 16,stride = 64):\n",
        "        self.c_in=c_in\n",
        "        self.context_window=context_window\n",
        "        self.target_window=target_window\n",
        "        self.patch_len=patch_len\n",
        "        self.stride=stride\n",
        "        self.patchtst_model = PatchTST(c_in=self.c_in, context_window=self.context_window, target_window=self.target_window, patch_len=self.patch_len, stride=self.stride)\n",
        "\n",
        "    def model_architecture(self):\n",
        "        n=0\n",
        "        for x in self.patchtst_model.state_dict():\n",
        "            n=n+1\n",
        "            print(x)\n",
        "        print(\"layers= \",n)\n",
        "\n",
        "    def load_datasets(self,train_dataset=train_dataset,valid_dataset=valid_dataset,test_dataset=test_dataset):\n",
        "        self.train_dataset=train_dataset\n",
        "        self.valid_dataset=valid_dataset\n",
        "        self.test_dataset=test_dataset\n",
        "\n",
        "\n",
        "\n",
        "    def model_hyperparameters(self,batch_size=128*8,lr=0.00001,epochs=100,cuda=True,Dataloader=DataLoader):\n",
        "        self.batch_size = batch_size\n",
        "        self.lr=lr\n",
        "        self.epochs=epochs\n",
        "\n",
        "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        self.valid_dataloader = DataLoader(self.valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "        if cuda:\n",
        "            self.patchtst_model.to(\"cuda\")\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.patchtst_model.parameters(), lr=lr)\n",
        "        self.loss = torch.nn.MSELoss()\n",
        "\n",
        "    def model_execute(self,):\n",
        "        self.load_datasets()\n",
        "        self.model_hyperparameters()\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            iter_count = 0\n",
        "            total_loss = 0\n",
        "            train_steps=0\n",
        "\n",
        "            for train_x, train_y in tqdm(self.train_dataloader):\n",
        "                train_x = train_x.to(\"cuda\")\n",
        "                train_y = train_y.to(\"cuda\")\n",
        "\n",
        "                pred_y = self.patchtst_model(train_x)\n",
        "                loss_t = self.loss(pred_y, train_y)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss_t.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss_t.item()\n",
        "                iter_count += 1\n",
        "                train_steps += 1\n",
        "\n",
        "            valid_iter_count = 0\n",
        "            valid_total_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for valid_x, valid_y in self.valid_dataloader:\n",
        "                    valid_x = valid_x.to(\"cuda\")\n",
        "                    valid_y = valid_y.to(\"cuda\")\n",
        "                    pred_y = self.patchtst_model(valid_x)\n",
        "                    loss_v = self.loss(pred_y, valid_y)\n",
        "                    valid_total_loss += loss_v.item()\n",
        "                    valid_iter_count += 1\n",
        "\n",
        "                total_loss /= iter_count\n",
        "                valid_total_loss /= valid_iter_count\n",
        "                print(\"epoch: {} MSE loss: {:.4f} MSE valid loss: {:.4f}\".format(epoch, total_loss, valid_total_loss))\n",
        "\n",
        "\n",
        "\n",
        "    def plots(self,):\n",
        "\n",
        "\n",
        "    def metric(self,):\n",
        "\n",
        "\n",
        "    def feature_metric(self,):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "c4be2e6f",
      "metadata": {
        "id": "c4be2e6f"
      },
      "outputs": [],
      "source": [
        "x=model_run(model=PatchTST,c_in = train_dataset.num_feat,context_window = 336,target_window = 96,patch_len = 16,stride = 64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.model_execute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "PRq_t36mOl4T",
        "outputId": "9a9906e8-8b37-4f11-ec37-929ffbbb4d05"
      },
      "id": "PRq_t36mOl4T",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41/41 [00:02<00:00, 17.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 MSE loss: 0.3357 MSE valid loss: 0.2526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 8/41 [00:00<00:01, 17.73it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-940d38bb06d7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-3307a0914c98>\u001b[0m in \u001b[0;36mmodel_execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mtrain_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tGYsv8-jOpOy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "d4331d8f-131d-4d4c-ae0e-f15e11f3b624"
      },
      "id": "tGYsv8-jOpOy",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-16816b69398f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RnxOeBvKRnQ-"
      },
      "id": "RnxOeBvKRnQ-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}