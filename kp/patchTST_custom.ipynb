{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ed103c66",
      "metadata": {
        "id": "ed103c66"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5522d111",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "5522d111",
        "outputId": "c8fa6bcd-ac5d-4f0d-96a9-b22fe97b3e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Date Time  p (mbar)  T (degC)  rh (%)  sh (g/kg)  Tpot (K)  \\\n",
              "0      01.01.2020 00:10:04   1019.28     -0.02    96.7       3.61    271.66   \n",
              "1      01.01.2020 00:20:04   1019.19      0.04    96.6       3.62    271.72   \n",
              "2      01.01.2020 00:30:04   1019.11      0.10    97.2       3.66    271.78   \n",
              "3      01.01.2020 00:40:04   1019.04      0.13    97.7       3.68    271.82   \n",
              "4      01.01.2020 00:50:04   1018.98      0.00    97.9       3.66    271.69   \n",
              "...                    ...       ...       ...     ...        ...       ...   \n",
              "52529  31.12.2020 23:20:00    988.15      2.16    85.8       3.86    276.25   \n",
              "52530  31.12.2020 23:30:00    988.13      2.15    88.6       3.99    276.24   \n",
              "52531  31.12.2020 23:40:00    988.07      2.03    88.8       3.96    276.13   \n",
              "52532  31.12.2020 23:50:00    988.08      1.93    88.5       3.92    276.03   \n",
              "52533  01.01.2021 00:00:00    988.07      1.87    89.9       3.97    275.97   \n",
              "\n",
              "       Tdew (degC)  VPmax (mbar)  VPact (mbar)  VPdef (mbar)  ...  \\\n",
              "0            -0.48          6.10          5.90          0.20  ...   \n",
              "1            -0.43          6.13          5.92          0.21  ...   \n",
              "2            -0.29          6.15          5.98          0.17  ...   \n",
              "3            -0.19          6.16          6.02          0.14  ...   \n",
              "4            -0.29          6.11          5.98          0.13  ...   \n",
              "...            ...           ...           ...           ...  ...   \n",
              "52529         0.04          7.14          6.13          1.01  ...   \n",
              "52530         0.47          7.13          6.32          0.81  ...   \n",
              "52531         0.39          7.07          6.28          0.79  ...   \n",
              "52532         0.24          7.02          6.22          0.81  ...   \n",
              "52533         0.40          6.99          6.29          0.71  ...   \n",
              "\n",
              "       ST008 (degC)  ST016 (degC)  ST032 (degC)  ST064 (degC)  ST128 (degC)  \\\n",
              "0              2.08          2.88          3.90          6.14          8.81   \n",
              "1              2.07          2.88          3.90          6.14          8.80   \n",
              "2              2.06          2.87          3.90          6.14          8.81   \n",
              "3              2.06          2.87          3.90          6.13          8.81   \n",
              "4              2.05          2.87          3.90          6.13          8.80   \n",
              "...             ...           ...           ...           ...           ...   \n",
              "52529          2.65          3.24          4.16          6.09          8.67   \n",
              "52530          2.66          3.24          4.16          6.09          8.67   \n",
              "52531          2.66          3.24          4.16          6.09          8.67   \n",
              "52532          2.67          3.25          4.16          6.09          8.67   \n",
              "52533          2.67          3.25          4.16          6.09          8.67   \n",
              "\n",
              "       SM008 (%)  SM016 (%)  SM032 (%)  SM064 (%)  SM128 (%)  \n",
              "0          32.50      30.09      31.64      21.55      29.76  \n",
              "1          32.50      30.09      31.64      21.55      29.76  \n",
              "2          32.50      30.09      31.64      21.55      29.76  \n",
              "3          32.50      30.09      31.64      21.55      29.76  \n",
              "4          32.50      30.09      31.64      21.55      29.76  \n",
              "...          ...        ...        ...        ...        ...  \n",
              "52529      37.31      33.10      33.01      30.75      31.80  \n",
              "52530      37.32      33.10      33.01      30.75      31.80  \n",
              "52531      37.32      33.10      33.01      30.75      31.80  \n",
              "52532      37.32      33.09      33.01      30.76      31.80  \n",
              "52533      37.32      33.09      33.01      30.76      31.80  \n",
              "\n",
              "[52534 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e142091-d1a6-49da-af4e-18b44fc21549\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date Time</th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>Tpot (K)</th>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>...</th>\n",
              "      <th>ST008 (degC)</th>\n",
              "      <th>ST016 (degC)</th>\n",
              "      <th>ST032 (degC)</th>\n",
              "      <th>ST064 (degC)</th>\n",
              "      <th>ST128 (degC)</th>\n",
              "      <th>SM008 (%)</th>\n",
              "      <th>SM016 (%)</th>\n",
              "      <th>SM032 (%)</th>\n",
              "      <th>SM064 (%)</th>\n",
              "      <th>SM128 (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01.01.2020 00:10:04</td>\n",
              "      <td>1019.28</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>96.7</td>\n",
              "      <td>3.61</td>\n",
              "      <td>271.66</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>6.10</td>\n",
              "      <td>5.90</td>\n",
              "      <td>0.20</td>\n",
              "      <td>...</td>\n",
              "      <td>2.08</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.14</td>\n",
              "      <td>8.81</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01.01.2020 00:20:04</td>\n",
              "      <td>1019.19</td>\n",
              "      <td>0.04</td>\n",
              "      <td>96.6</td>\n",
              "      <td>3.62</td>\n",
              "      <td>271.72</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>6.13</td>\n",
              "      <td>5.92</td>\n",
              "      <td>0.21</td>\n",
              "      <td>...</td>\n",
              "      <td>2.07</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.14</td>\n",
              "      <td>8.80</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01.01.2020 00:30:04</td>\n",
              "      <td>1019.11</td>\n",
              "      <td>0.10</td>\n",
              "      <td>97.2</td>\n",
              "      <td>3.66</td>\n",
              "      <td>271.78</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>6.15</td>\n",
              "      <td>5.98</td>\n",
              "      <td>0.17</td>\n",
              "      <td>...</td>\n",
              "      <td>2.06</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.14</td>\n",
              "      <td>8.81</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01.01.2020 00:40:04</td>\n",
              "      <td>1019.04</td>\n",
              "      <td>0.13</td>\n",
              "      <td>97.7</td>\n",
              "      <td>3.68</td>\n",
              "      <td>271.82</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>6.16</td>\n",
              "      <td>6.02</td>\n",
              "      <td>0.14</td>\n",
              "      <td>...</td>\n",
              "      <td>2.06</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.13</td>\n",
              "      <td>8.81</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01.01.2020 00:50:04</td>\n",
              "      <td>1018.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>97.9</td>\n",
              "      <td>3.66</td>\n",
              "      <td>271.69</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>6.11</td>\n",
              "      <td>5.98</td>\n",
              "      <td>0.13</td>\n",
              "      <td>...</td>\n",
              "      <td>2.05</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.90</td>\n",
              "      <td>6.13</td>\n",
              "      <td>8.80</td>\n",
              "      <td>32.50</td>\n",
              "      <td>30.09</td>\n",
              "      <td>31.64</td>\n",
              "      <td>21.55</td>\n",
              "      <td>29.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52529</th>\n",
              "      <td>31.12.2020 23:20:00</td>\n",
              "      <td>988.15</td>\n",
              "      <td>2.16</td>\n",
              "      <td>85.8</td>\n",
              "      <td>3.86</td>\n",
              "      <td>276.25</td>\n",
              "      <td>0.04</td>\n",
              "      <td>7.14</td>\n",
              "      <td>6.13</td>\n",
              "      <td>1.01</td>\n",
              "      <td>...</td>\n",
              "      <td>2.65</td>\n",
              "      <td>3.24</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.31</td>\n",
              "      <td>33.10</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.75</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52530</th>\n",
              "      <td>31.12.2020 23:30:00</td>\n",
              "      <td>988.13</td>\n",
              "      <td>2.15</td>\n",
              "      <td>88.6</td>\n",
              "      <td>3.99</td>\n",
              "      <td>276.24</td>\n",
              "      <td>0.47</td>\n",
              "      <td>7.13</td>\n",
              "      <td>6.32</td>\n",
              "      <td>0.81</td>\n",
              "      <td>...</td>\n",
              "      <td>2.66</td>\n",
              "      <td>3.24</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.32</td>\n",
              "      <td>33.10</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.75</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52531</th>\n",
              "      <td>31.12.2020 23:40:00</td>\n",
              "      <td>988.07</td>\n",
              "      <td>2.03</td>\n",
              "      <td>88.8</td>\n",
              "      <td>3.96</td>\n",
              "      <td>276.13</td>\n",
              "      <td>0.39</td>\n",
              "      <td>7.07</td>\n",
              "      <td>6.28</td>\n",
              "      <td>0.79</td>\n",
              "      <td>...</td>\n",
              "      <td>2.66</td>\n",
              "      <td>3.24</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.32</td>\n",
              "      <td>33.10</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.75</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52532</th>\n",
              "      <td>31.12.2020 23:50:00</td>\n",
              "      <td>988.08</td>\n",
              "      <td>1.93</td>\n",
              "      <td>88.5</td>\n",
              "      <td>3.92</td>\n",
              "      <td>276.03</td>\n",
              "      <td>0.24</td>\n",
              "      <td>7.02</td>\n",
              "      <td>6.22</td>\n",
              "      <td>0.81</td>\n",
              "      <td>...</td>\n",
              "      <td>2.67</td>\n",
              "      <td>3.25</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.32</td>\n",
              "      <td>33.09</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.76</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52533</th>\n",
              "      <td>01.01.2021 00:00:00</td>\n",
              "      <td>988.07</td>\n",
              "      <td>1.87</td>\n",
              "      <td>89.9</td>\n",
              "      <td>3.97</td>\n",
              "      <td>275.97</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.99</td>\n",
              "      <td>6.29</td>\n",
              "      <td>0.71</td>\n",
              "      <td>...</td>\n",
              "      <td>2.67</td>\n",
              "      <td>3.25</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.09</td>\n",
              "      <td>8.67</td>\n",
              "      <td>37.32</td>\n",
              "      <td>33.09</td>\n",
              "      <td>33.01</td>\n",
              "      <td>30.76</td>\n",
              "      <td>31.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52534 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e142091-d1a6-49da-af4e-18b44fc21549')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4e142091-d1a6-49da-af4e-18b44fc21549 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4e142091-d1a6-49da-af4e-18b44fc21549');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0b5aba9b-8616-41e1-a969-cd6f8ab94447\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0b5aba9b-8616-41e1-a969-cd6f8ab94447')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0b5aba9b-8616-41e1-a969-cd6f8ab94447 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/')\n",
        "\n",
        "\n",
        "df=pd.read_csv('weather.csv')\n",
        "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bffcb3d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bffcb3d9",
        "outputId": "bfed6676-2f66-4795-b700-e634f3e570a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndf=pd.read_csv('weather.csv',index_col=None)\\ndf.drop('Unnamed: 0',axis=1,inplace=True)\\ndf\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "'''\n",
        "df=pd.read_csv('weather.csv',index_col=None)\n",
        "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "df\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class weather_data(torch.utils.data.Dataset):\n",
        "    def __init__(self, df,scale=True, mode=\"train\", seq_len=336*8, pred_len=96):\n",
        "        super().__init__()\n",
        "        self.df = df.iloc[:,1:]\n",
        "        # time_stamp = df.iloc[:,0]\n",
        "\n",
        "        assert mode in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[mode]\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        dataset_len=len(df)\n",
        "\n",
        "        border1s = [0, int(round(0.8*dataset_len,0)) - self.seq_len, int(round(0.8*dataset_len,0)) + int(round(0.1*dataset_len,0)) - self.seq_len]\n",
        "        border2s = [int(round(0.8*dataset_len,0)), int(round(0.8*dataset_len,0)) + int(round(0.1*dataset_len,0)), int(round(0.8*dataset_len,0)) + int(round(0.1*dataset_len,0)) + int(round(0.1*dataset_len,0))]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "\n",
        "        if scale:\n",
        "          scaler = StandardScaler()\n",
        "          self.df = self.df.to_numpy(dtype=np.float32)\n",
        "          self.df = scaler.fit_transform(self.df)\n",
        "\n",
        "        else:\n",
        "          self.df = self.df.to_numpy(dtype=np.float32)\n",
        "        # time_stamp = time_stamp.to_numpy()\n",
        "\n",
        "        self.data_x = self.df[border1: border2, :]\n",
        "        self.data_y = self.df[border1: border2, -1]\n",
        "\n",
        "        # self.data_stamp = time_stamp[border1: border2]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end\n",
        "        r_end = r_begin + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        return seq_x, seq_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1"
      ],
      "metadata": {
        "id": "IMhozIdUX4fg"
      },
      "id": "IMhozIdUX4fg",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "21df0973",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "21df0973",
        "outputId": "ce542009-199a-44f8-99e7-9364b6ca9cc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass weather_data(Dataset):\\n\\n    def __init__(self,data,X_seq_len,y_seq_len):\\n        self.data = data.to_numpy(dtype=np.float32)\\n        self.X_seq_len=X_seq_len\\n        self.y_seq_len=y_seq_len\\n\\n    def __len__(self):\\n        return len(self.data)-self.X_seq_len-self.y_seq_len+1\\n\\n    def __getitem__(self,idx):\\n        X = self.data[idx:idx + self.X_seq_len]\\n        y = self.data[idx + self.X_seq_len:idx + self.X_seq_len + self.y_seq_len]\\n        return torch.tensor(X), torch.tensor(y)\\n        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "'''\n",
        "class weather_data(Dataset):\n",
        "\n",
        "    def __init__(self,data,X_seq_len,y_seq_len):\n",
        "        self.data = data.to_numpy(dtype=np.float32)\n",
        "        self.X_seq_len=X_seq_len\n",
        "        self.y_seq_len=y_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)-self.X_seq_len-self.y_seq_len+1\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        X = self.data[idx:idx + self.X_seq_len]\n",
        "        y = self.data[idx + self.X_seq_len:idx + self.X_seq_len + self.y_seq_len]\n",
        "        return torch.tensor(X), torch.tensor(y)\n",
        "        '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = weather_data(df=df)\n",
        "# test_dataset = weather_data(test_data, test_target, sequence_length)\n",
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds8C5Wv1TZJy",
        "outputId": "133406fb-dc83-4180-d22d-8a66016282dc"
      },
      "id": "Ds8C5Wv1TZJy",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39244"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataloader = weather_data(df=df,mode=\"val\")\n",
        "len(valid_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXCfSCPZXRm6",
        "outputId": "501b1d8b-d059-4c3f-8ad6-a563e873db34"
      },
      "id": "eXCfSCPZXRm6",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5158"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b=train_dataset[0]\n",
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz5Bj4w0YjjL",
        "outputId": "9c274478-e9b3-47ce-95b4-e4968408a06c"
      },
      "id": "Xz5Bj4w0YjjL",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2688, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFl6fXYZZvkI",
        "outputId": "2013b602-5cbb-4848-84e2-97d300dd80bf"
      },
      "id": "eFl6fXYZZvkI",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96,)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "70a18f29",
      "metadata": {
        "id": "70a18f29"
      },
      "outputs": [],
      "source": [
        "#Implementation of head layer\n",
        "\n",
        "class Flatten_Head(torch.nn.Module):\n",
        "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
        "        super().__init__()\n",
        "        self.n_vars = n_vars\n",
        "        self.flatten = torch.nn.Flatten(start_dim=-3)\n",
        "        self.linear = torch.nn.Linear(nf * n_vars, target_window)\n",
        "        self.dropout = torch.nn.Dropout(head_dropout)\n",
        "\n",
        "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
        "        x = self.flatten(x)                               # x: [bs x nvars * d_model * patch_num]\n",
        "        x = self.linear(x)                                # x: [bs x target_window]\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "28abbfa0",
      "metadata": {
        "id": "28abbfa0"
      },
      "outputs": [],
      "source": [
        "\n",
        "#implementation of utils\n",
        "class RevIN(torch.nn.Module):\n",
        "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False, target_idx=-1):\n",
        "        super(RevIN, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.subtract_last = subtract_last\n",
        "        self.target_idx = target_idx\n",
        "        if self.affine:\n",
        "            self._init_params()\n",
        "\n",
        "    def forward(self, x, mode):\n",
        "        if mode == \"norm\":\n",
        "            self._get_statistics(x)\n",
        "            x = self._normalize(x)\n",
        "        elif mode == \"denorm\":\n",
        "            x = self._denormalize(x)\n",
        "        else: raise AssertionError\n",
        "        return x\n",
        "\n",
        "    def _init_params(self):\n",
        "        self.affine_weight = torch.nn.Parameter(torch.ones(self.num_features))\n",
        "        self.affine_bias = torch.nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "    def _get_statistics(self, x):\n",
        "        dim2reduce = tuple(range(1, x.ndim-1))\n",
        "        if self.subtract_last:\n",
        "            self.last = x[:,-1,:].unsqueeze(1)\n",
        "        else:\n",
        "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "    def _normalize(self, x):\n",
        "        if self.subtract_last:\n",
        "            x = x - self.last\n",
        "        else:\n",
        "            x = x - self.mean\n",
        "        x = x / self.stdev\n",
        "        if self.affine:\n",
        "            x = x * self.affine_weight\n",
        "            x = x + self.affine_bias\n",
        "        return x\n",
        "\n",
        "    def _denormalize(self, x):\n",
        "        if self.affine:\n",
        "            x = x - self.affine_bias\n",
        "            x = x / (self.affine_weight + self.eps*self.eps)\n",
        "        x = x * self.stdev[:, :, self.target_idx]\n",
        "        if self.subtract_last:\n",
        "            x = x + self.last[:, :, self.target_idx]\n",
        "        else:\n",
        "            x = x + self.mean[:, :, self.target_idx]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transpose(torch.nn.Module):\n",
        "    def __init__(self, *dims, contiguous=False):\n",
        "        super().__init__()\n",
        "        self.dims, self.contiguous = dims, contiguous\n",
        "    def forward(self, x):\n",
        "        if self.contiguous:\n",
        "            return x.transpose(*self.dims).contiguous()\n",
        "        else:\n",
        "            return x.transpose(*self.dims)\n",
        "\n",
        "\n",
        "def positional_encoding(q_len, d_model):\n",
        "    W_pos = torch.empty((q_len, d_model))\n",
        "    torch.nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    return torch.nn.Parameter(W_pos, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "e1f414ee",
      "metadata": {
        "id": "e1f414ee"
      },
      "outputs": [],
      "source": [
        "#Implementation of PatchTST Encorder layer\n",
        "class TSTiEncoder(torch.nn.Module):  #i means channel-independent\n",
        "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
        "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
        "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., store_attn=False,\n",
        "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
        "                 verbose=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.patch_num = patch_num\n",
        "        self.patch_len = patch_len\n",
        "        # Input encoding\n",
        "        q_len = patch_num\n",
        "        self.W_P = torch.nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
        "        self.seq_len = q_len\n",
        "        # Positional encoding\n",
        "        self.W_pos = positional_encoding(q_len, d_model)\n",
        "        # Residual dropout\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        # Encoder\n",
        "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                   pre_norm=pre_norm, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
        "\n",
        "    def forward(self, x):                                              # x: [bs x nvars x patch_len x patch_num]\n",
        "\n",
        "        n_vars = x.shape[1]\n",
        "        # Input encoding\n",
        "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
        "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
        "\n",
        "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
        "        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
        "\n",
        "        # Encoder\n",
        "        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
        "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
        "        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "# Cell\n",
        "class TSTEncoder(torch.nn.Module):\n",
        "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None,\n",
        "                        norm='BatchNorm', attn_dropout=0., dropout=0.,\n",
        "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
        "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                                      res_attention=res_attention,\n",
        "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
        "        self.res_attention = res_attention\n",
        "\n",
        "    def forward(self, src, key_padding_mask=None, attn_mask=None):\n",
        "        output = src\n",
        "        scores = None\n",
        "        if self.res_attention:\n",
        "            for mod in self.layers:\n",
        "                output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n",
        "        else:\n",
        "            for mod in self.layers:\n",
        "                output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "class TSTEncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
        "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, res_attention=False, pre_norm=False):\n",
        "        super().__init__()\n",
        "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        # Multi-Head attention\n",
        "        self.res_attention = res_attention\n",
        "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_attn = torch.nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_attn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_attn = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "        # Position-wise Feed-Forward\n",
        "        self.ff = torch.nn.Sequential(torch.nn.Linear(d_model, d_ff, bias=bias),\n",
        "                                torch.nn.GELU(),\n",
        "                                torch.nn.Dropout(dropout),\n",
        "                                torch.nn.Linear(d_ff, d_model, bias=bias))\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_ffn = torch.nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_ffn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_ffn = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.store_attn = store_attn\n",
        "\n",
        "\n",
        "    def forward(self, src, prev=None, key_padding_mask=None, attn_mask=None):\n",
        "\n",
        "        # Multi-Head attention sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "        ## Multi-Head attention\n",
        "        if self.res_attention:\n",
        "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        if self.store_attn:\n",
        "            self.attn = attn\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "        ## Position-wise Feed-Forward\n",
        "        src2 = self.ff(src)\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "\n",
        "        if self.res_attention:\n",
        "            return src, scores\n",
        "        else:\n",
        "            return src\n",
        "\n",
        "\n",
        "class _MultiheadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
        "        super().__init__()\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
        "\n",
        "        self.W_Q = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_K = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_V = torch.nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
        "\n",
        "        # Scaled Dot-Product Attention (multiple heads)\n",
        "        self.res_attention = res_attention\n",
        "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
        "\n",
        "        # Poject output\n",
        "        self.to_out = torch.nn.Sequential(torch.nn.Linear(n_heads * d_v, d_model), torch.nn.Dropout(proj_dropout))\n",
        "\n",
        "    def forward(self, Q, K=None, V=None, prev=None,\n",
        "                key_padding_mask=None, attn_mask=None):\n",
        "\n",
        "        bs = Q.size(0)\n",
        "        if K is None: K = Q\n",
        "        if V is None: V = Q\n",
        "\n",
        "        # Linear (+ split in multiple heads)\n",
        "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
        "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
        "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
        "\n",
        "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
        "        if self.res_attention:\n",
        "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # back to the original inputs dimensions\n",
        "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
        "        output = self.to_out(output)\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights\n",
        "\n",
        "\n",
        "class _ScaledDotProductAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
        "        super().__init__()\n",
        "        self.attn_dropout = torch.nn.Dropout(attn_dropout)\n",
        "        self.res_attention = res_attention\n",
        "        head_dim = d_model // n_heads\n",
        "        self.scale = torch.nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
        "        self.lsa = lsa\n",
        "\n",
        "    def forward(self, q, k, v, prev=None, key_padding_mask=None, attn_mask=None):\n",
        "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
        "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # Add pre-softmax attention scores from the previous layer (optional)\n",
        "        if prev is not None: attn_scores = attn_scores + prev\n",
        "\n",
        "        # Attention mask (optional)\n",
        "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
        "            if attn_mask.dtype == torch.bool:\n",
        "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
        "            else:\n",
        "                attn_scores += attn_mask\n",
        "\n",
        "        # Key padding mask (optional)\n",
        "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
        "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
        "\n",
        "        # normalize the attention weights\n",
        "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # compute the new values given the attention weights\n",
        "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "02761932",
      "metadata": {
        "id": "02761932"
      },
      "outputs": [],
      "source": [
        "#Implementation of PatchTST\n",
        "\n",
        "class PatchTST(torch.nn.Module):\n",
        "    def __init__(self, c_in, context_window, target_window, patch_len, stride, max_seq_len=1024,\n",
        "                 n_layers=3, d_model=16, n_heads=4, d_k=None, d_v=None,\n",
        "                 d_ff=128, attn_dropout=0.0, dropout=0.3, key_padding_mask='auto',\n",
        "                 padding_var=None, attn_mask=None, res_attention=True, pre_norm=False, store_attn=False,\n",
        "                 head_dropout = 0.0, padding_patch = \"end\",\n",
        "                 revin = True, affine = False, subtract_last = False,\n",
        "                 verbose=False, target_idx=-1, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.revin = revin\n",
        "        if revin:\n",
        "            self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last, target_idx=target_idx)\n",
        "\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding_patch = padding_patch\n",
        "        patch_num = int((context_window - patch_len)/stride + 1)\n",
        "\n",
        "        if padding_patch == \"end\":\n",
        "            self.padding_patch_layer = torch.nn.ReplicationPad1d((0, stride))\n",
        "            patch_num += 1\n",
        "\n",
        "        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
        "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
        "                                attn_dropout=attn_dropout, dropout=dropout, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
        "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
        "                                verbose=verbose, **kwargs)\n",
        "\n",
        "        self.head_nf = d_model * patch_num\n",
        "        self.n_vars = c_in\n",
        "\n",
        "        self.head = Flatten_Head(self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
        "\n",
        "    def forward(self, z):                                                                   # z: [bs x seq_len Ã— nvars]\n",
        "        # instance norm\n",
        "        if self.revin:\n",
        "            z = self.revin_layer(z, 'norm')\n",
        "            z = z.permute(0,2,1)                                                            # z: [bs x nvars Ã— seq_len]\n",
        "\n",
        "        # do patching\n",
        "        if self.padding_patch == 'end':\n",
        "            z = self.padding_patch_layer(z)\n",
        "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
        "        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
        "\n",
        "        # model\n",
        "        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
        "        z = self.head(z)                                                                    # z: [bs x target_window]\n",
        "\n",
        "        # denorm\n",
        "        if self.revin:\n",
        "            z = self.revin_layer(z, 'denorm')\n",
        "        return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "b7f6dca5",
      "metadata": {
        "id": "b7f6dca5"
      },
      "outputs": [],
      "source": [
        "#Linear model\n",
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, c_in, context_window, target_window):\n",
        "        super().__init__()\n",
        "        self.c_in = c_in\n",
        "        self.context_winsoq = context_window\n",
        "        self.target_window = target_window\n",
        "\n",
        "        self.flatten = torch.nn.Flatten(start_dim=-2)\n",
        "\n",
        "        self.linear = torch.nn.Linear(c_in * context_window, target_window)\n",
        "\n",
        "    def forward(self, x):                   # x: [bs x seq_len Ã— nvars]\n",
        "        x = self.flatten(x)                 # x: [bs x seq_len * nvars]\n",
        "        x = self.linear(x)                  # x: [bs x target_window]\n",
        "        return x\n",
        "\n",
        "\n",
        "class moving_avg(torch.nn.Module):\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # padding on the both ends of time series\n",
        "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        x = torch.cat([front, x, end], dim=1)\n",
        "        x = self.avg(x.permute(0, 2, 1))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class series_decomp(torch.nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean\n",
        "\n",
        "class DLinear(torch.nn.Module):\n",
        "    def __init__(self, c_in, context_window, target_window):\n",
        "        super().__init__()\n",
        "        # Decompsition Kernel Size\n",
        "        kernel_size = 25\n",
        "        self.decompsition = series_decomp(kernel_size)\n",
        "        self.flatten_Seasonal = torch.nn.Flatten(start_dim=-2)\n",
        "        self.flatten_Trend = torch.nn.Flatten(start_dim=-2)\n",
        "\n",
        "        self.Linear_Seasonal = torch.nn.Linear(c_in * context_window, target_window)\n",
        "        self.Linear_Trend = torch.nn.Linear(c_in * context_window, target_window)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Input length, Channel]\n",
        "        seasonal_init, trend_init = self.decompsition(x)\n",
        "        seasonal_init = self.flatten_Seasonal(x)\n",
        "        trend_init = self.flatten_Trend(x)\n",
        "\n",
        "        seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
        "        trend_output = self.Linear_Trend(trend_init)\n",
        "\n",
        "        x = seasonal_output + trend_output\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "0d28fdc7",
      "metadata": {
        "id": "0d28fdc7"
      },
      "outputs": [],
      "source": [
        "c_in = df.shape[1] - 1\n",
        "context_window = 336*8\n",
        "target_window = 96\n",
        "patch_len = 16\n",
        "stride = 64\n",
        "\n",
        "patchtst_model = PatchTST(c_in=c_in, context_window=context_window, target_window=target_window, patch_len=patch_len, stride=stride)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "0a3b812c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a3b812c",
        "outputId": "3c12036c-73fd-4fbb-b685-d671a68a0ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "backbone.W_pos\n",
            "backbone.W_P.weight\n",
            "backbone.W_P.bias\n",
            "backbone.encoder.layers.0.self_attn.W_Q.weight\n",
            "backbone.encoder.layers.0.self_attn.W_Q.bias\n",
            "backbone.encoder.layers.0.self_attn.W_K.weight\n",
            "backbone.encoder.layers.0.self_attn.W_K.bias\n",
            "backbone.encoder.layers.0.self_attn.W_V.weight\n",
            "backbone.encoder.layers.0.self_attn.W_V.bias\n",
            "backbone.encoder.layers.0.self_attn.sdp_attn.scale\n",
            "backbone.encoder.layers.0.self_attn.to_out.0.weight\n",
            "backbone.encoder.layers.0.self_attn.to_out.0.bias\n",
            "backbone.encoder.layers.0.norm_attn.1.weight\n",
            "backbone.encoder.layers.0.norm_attn.1.bias\n",
            "backbone.encoder.layers.0.norm_attn.1.running_mean\n",
            "backbone.encoder.layers.0.norm_attn.1.running_var\n",
            "backbone.encoder.layers.0.norm_attn.1.num_batches_tracked\n",
            "backbone.encoder.layers.0.ff.0.weight\n",
            "backbone.encoder.layers.0.ff.0.bias\n",
            "backbone.encoder.layers.0.ff.3.weight\n",
            "backbone.encoder.layers.0.ff.3.bias\n",
            "backbone.encoder.layers.0.norm_ffn.1.weight\n",
            "backbone.encoder.layers.0.norm_ffn.1.bias\n",
            "backbone.encoder.layers.0.norm_ffn.1.running_mean\n",
            "backbone.encoder.layers.0.norm_ffn.1.running_var\n",
            "backbone.encoder.layers.0.norm_ffn.1.num_batches_tracked\n",
            "backbone.encoder.layers.1.self_attn.W_Q.weight\n",
            "backbone.encoder.layers.1.self_attn.W_Q.bias\n",
            "backbone.encoder.layers.1.self_attn.W_K.weight\n",
            "backbone.encoder.layers.1.self_attn.W_K.bias\n",
            "backbone.encoder.layers.1.self_attn.W_V.weight\n",
            "backbone.encoder.layers.1.self_attn.W_V.bias\n",
            "backbone.encoder.layers.1.self_attn.sdp_attn.scale\n",
            "backbone.encoder.layers.1.self_attn.to_out.0.weight\n",
            "backbone.encoder.layers.1.self_attn.to_out.0.bias\n",
            "backbone.encoder.layers.1.norm_attn.1.weight\n",
            "backbone.encoder.layers.1.norm_attn.1.bias\n",
            "backbone.encoder.layers.1.norm_attn.1.running_mean\n",
            "backbone.encoder.layers.1.norm_attn.1.running_var\n",
            "backbone.encoder.layers.1.norm_attn.1.num_batches_tracked\n",
            "backbone.encoder.layers.1.ff.0.weight\n",
            "backbone.encoder.layers.1.ff.0.bias\n",
            "backbone.encoder.layers.1.ff.3.weight\n",
            "backbone.encoder.layers.1.ff.3.bias\n",
            "backbone.encoder.layers.1.norm_ffn.1.weight\n",
            "backbone.encoder.layers.1.norm_ffn.1.bias\n",
            "backbone.encoder.layers.1.norm_ffn.1.running_mean\n",
            "backbone.encoder.layers.1.norm_ffn.1.running_var\n",
            "backbone.encoder.layers.1.norm_ffn.1.num_batches_tracked\n",
            "backbone.encoder.layers.2.self_attn.W_Q.weight\n",
            "backbone.encoder.layers.2.self_attn.W_Q.bias\n",
            "backbone.encoder.layers.2.self_attn.W_K.weight\n",
            "backbone.encoder.layers.2.self_attn.W_K.bias\n",
            "backbone.encoder.layers.2.self_attn.W_V.weight\n",
            "backbone.encoder.layers.2.self_attn.W_V.bias\n",
            "backbone.encoder.layers.2.self_attn.sdp_attn.scale\n",
            "backbone.encoder.layers.2.self_attn.to_out.0.weight\n",
            "backbone.encoder.layers.2.self_attn.to_out.0.bias\n",
            "backbone.encoder.layers.2.norm_attn.1.weight\n",
            "backbone.encoder.layers.2.norm_attn.1.bias\n",
            "backbone.encoder.layers.2.norm_attn.1.running_mean\n",
            "backbone.encoder.layers.2.norm_attn.1.running_var\n",
            "backbone.encoder.layers.2.norm_attn.1.num_batches_tracked\n",
            "backbone.encoder.layers.2.ff.0.weight\n",
            "backbone.encoder.layers.2.ff.0.bias\n",
            "backbone.encoder.layers.2.ff.3.weight\n",
            "backbone.encoder.layers.2.ff.3.bias\n",
            "backbone.encoder.layers.2.norm_ffn.1.weight\n",
            "backbone.encoder.layers.2.norm_ffn.1.bias\n",
            "backbone.encoder.layers.2.norm_ffn.1.running_mean\n",
            "backbone.encoder.layers.2.norm_ffn.1.running_var\n",
            "backbone.encoder.layers.2.norm_ffn.1.num_batches_tracked\n",
            "head.linear.weight\n",
            "head.linear.bias\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "n=0\n",
        "for x in patchtst_model.state_dict():\n",
        "    n=n+1\n",
        "    print(x)\n",
        "n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=128*8, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeuuOTvsL8mM",
        "outputId": "2d94f406-258e-4e5b-dd5e-c3e38448cd94"
      },
      "id": "CeuuOTvsL8mM",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f1876b9e740>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataloader = DataLoader(train_dataset, batch_size=128*8, shuffle=True)\n",
        "valid_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFi97LNgXClw",
        "outputId": "bbfee9c8-5cca-4def-9aab-a2f56206c85e"
      },
      "id": "xFi97LNgXClw",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f1876b9d810>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "032b97a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "032b97a3",
        "outputId": "42265e9f-d8c4-4c12-ff72-f1680cf46f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 MSE loss: 2.4711 MSE valid loss: 2.3670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 MSE loss: 2.3728 MSE valid loss: 2.3238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 2 MSE loss: 2.3031 MSE valid loss: 2.3610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 3 MSE loss: 2.2803 MSE valid loss: 2.2453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 4 MSE loss: 2.2624 MSE valid loss: 2.1997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 5 MSE loss: 2.1585 MSE valid loss: 2.1445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 6 MSE loss: 2.1895 MSE valid loss: 2.1280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 7 MSE loss: 2.1237 MSE valid loss: 2.0783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 8 MSE loss: 2.0623 MSE valid loss: 2.1345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 9 MSE loss: 2.0466 MSE valid loss: 2.0118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10 MSE loss: 2.0227 MSE valid loss: 2.0520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 11 MSE loss: 2.0037 MSE valid loss: 1.9566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 12 MSE loss: 1.9714 MSE valid loss: 1.9588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 13 MSE loss: 1.9183 MSE valid loss: 1.9308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 14 MSE loss: 1.9445 MSE valid loss: 1.9126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 15 MSE loss: 1.9031 MSE valid loss: 1.8871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 16 MSE loss: 1.9341 MSE valid loss: 1.8662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 17 MSE loss: 1.8483 MSE valid loss: 1.8407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 18 MSE loss: 1.8009 MSE valid loss: 1.7864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 19 MSE loss: 1.8082 MSE valid loss: 1.7959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 20 MSE loss: 1.7612 MSE valid loss: 1.7479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 21 MSE loss: 1.7652 MSE valid loss: 1.7815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 22 MSE loss: 1.7702 MSE valid loss: 1.7048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 23 MSE loss: 1.8145 MSE valid loss: 1.6927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 24 MSE loss: 1.7719 MSE valid loss: 1.6762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 25 MSE loss: 1.6724 MSE valid loss: 1.6567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 26 MSE loss: 1.6851 MSE valid loss: 1.7322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 27 MSE loss: 1.6662 MSE valid loss: 1.6603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 28 MSE loss: 1.6256 MSE valid loss: 1.6512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 29 MSE loss: 1.6441 MSE valid loss: 1.6678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 30 MSE loss: 1.5981 MSE valid loss: 1.5936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 31 MSE loss: 1.6490 MSE valid loss: 1.5835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 32 MSE loss: 1.5811 MSE valid loss: 1.5705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 33 MSE loss: 1.6280 MSE valid loss: 1.5922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 34 MSE loss: 1.5905 MSE valid loss: 1.5805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 35 MSE loss: 1.5811 MSE valid loss: 1.5453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 36 MSE loss: 1.5428 MSE valid loss: 1.6214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 37 MSE loss: 1.5377 MSE valid loss: 1.6132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 38 MSE loss: 1.5262 MSE valid loss: 1.5214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 39 MSE loss: 1.5489 MSE valid loss: 1.5412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 40 MSE loss: 1.5117 MSE valid loss: 1.5078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 41 MSE loss: 1.5361 MSE valid loss: 1.5265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 42 MSE loss: 1.5262 MSE valid loss: 1.5219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 43 MSE loss: 1.4923 MSE valid loss: 1.5181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 44 MSE loss: 1.5438 MSE valid loss: 1.5689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 45 MSE loss: 1.4840 MSE valid loss: 1.5636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 46 MSE loss: 1.5327 MSE valid loss: 1.5024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 47 MSE loss: 1.4728 MSE valid loss: 1.5278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 48 MSE loss: 1.4981 MSE valid loss: 1.5219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 49 MSE loss: 1.4920 MSE valid loss: 1.4597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 50 MSE loss: 1.4888 MSE valid loss: 1.4570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 51 MSE loss: 1.4564 MSE valid loss: 1.4829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 52 MSE loss: 1.5102 MSE valid loss: 1.4791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:24<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 53 MSE loss: 1.4786 MSE valid loss: 1.5059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 54 MSE loss: 1.4447 MSE valid loss: 1.4438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:24<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 55 MSE loss: 1.4414 MSE valid loss: 1.4690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 56 MSE loss: 1.4395 MSE valid loss: 1.4398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 57 MSE loss: 1.4337 MSE valid loss: 1.4637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:24<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 58 MSE loss: 1.4605 MSE valid loss: 1.4617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 59 MSE loss: 1.4889 MSE valid loss: 1.4571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 60 MSE loss: 1.4862 MSE valid loss: 1.4851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 61 MSE loss: 1.4274 MSE valid loss: 1.4537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 62 MSE loss: 1.4527 MSE valid loss: 1.4221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 63 MSE loss: 1.4492 MSE valid loss: 1.4487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 64 MSE loss: 1.4476 MSE valid loss: 1.4180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 65 MSE loss: 1.4167 MSE valid loss: 1.4448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 66 MSE loss: 1.4144 MSE valid loss: 1.4988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:24<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 67 MSE loss: 1.4419 MSE valid loss: 1.4410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 68 MSE loss: 1.4115 MSE valid loss: 1.4380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:24<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 69 MSE loss: 1.4092 MSE valid loss: 1.4650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 70 MSE loss: 1.4646 MSE valid loss: 1.4635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 71 MSE loss: 1.4069 MSE valid loss: 1.4633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 72 MSE loss: 1.4047 MSE valid loss: 1.4027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 73 MSE loss: 1.4320 MSE valid loss: 1.4015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 74 MSE loss: 1.4573 MSE valid loss: 1.4574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 75 MSE loss: 1.4006 MSE valid loss: 1.3987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 76 MSE loss: 1.3985 MSE valid loss: 1.4259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 77 MSE loss: 1.4526 MSE valid loss: 1.3967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 78 MSE loss: 1.4246 MSE valid loss: 1.3949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 79 MSE loss: 1.4236 MSE valid loss: 1.4213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 80 MSE loss: 1.3921 MSE valid loss: 1.3921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 81 MSE loss: 1.3917 MSE valid loss: 1.3908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:24<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 82 MSE loss: 1.4750 MSE valid loss: 1.4461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 83 MSE loss: 1.4169 MSE valid loss: 1.4185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 84 MSE loss: 1.4451 MSE valid loss: 1.3876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:24<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 85 MSE loss: 1.4440 MSE valid loss: 1.3868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 86 MSE loss: 1.4142 MSE valid loss: 1.4422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 87 MSE loss: 1.4419 MSE valid loss: 1.3839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 88 MSE loss: 1.3842 MSE valid loss: 1.3831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 89 MSE loss: 1.3831 MSE valid loss: 1.3818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 90 MSE loss: 1.4095 MSE valid loss: 1.4107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:24<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 91 MSE loss: 1.4385 MSE valid loss: 1.3802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 92 MSE loss: 1.4088 MSE valid loss: 1.3792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 93 MSE loss: 1.4079 MSE valid loss: 1.4065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 94 MSE loss: 1.4070 MSE valid loss: 1.4060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 95 MSE loss: 1.4066 MSE valid loss: 1.3768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 96 MSE loss: 1.4336 MSE valid loss: 1.4047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 97 MSE loss: 1.4040 MSE valid loss: 1.3754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 98 MSE loss: 1.4033 MSE valid loss: 1.4307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 99 MSE loss: 1.4034 MSE valid loss: 1.4019\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model=patchtst_model\n",
        "batch_size=128*8\n",
        "lr=0.00001\n",
        "epochs=100\n",
        "target_window=96\n",
        "\n",
        "model = model.to(\"cuda\")\n",
        "batch_size = batch_size\n",
        "train_datalen = len(train_dataset)\n",
        "train_dataloader = train_loader\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  iter_count = 0\n",
        "  total_loss = 0\n",
        "  train_steps=0\n",
        "\n",
        "  for train_x, train_y in tqdm(train_dataloader):\n",
        "    train_x = train_x.to(\"cuda\")\n",
        "    train_y = train_y.to(\"cuda\")\n",
        "\n",
        "    pred_y = model(train_x)\n",
        "    loss_t = loss(pred_y, train_y)\n",
        "    optimizer.zero_grad()\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss_t.item()\n",
        "    iter_count += 1\n",
        "    train_steps += 1\n",
        "\n",
        "  valid_iter_count = 0\n",
        "  valid_total_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for valid_x, valid_y in valid_dataloader:\n",
        "      valid_x = valid_x.to(\"cuda\")\n",
        "      valid_y = valid_y.to(\"cuda\")\n",
        "      pred_y = model(valid_x)\n",
        "      loss_v = loss(pred_y, valid_y)\n",
        "      valid_total_loss += loss_v.item()\n",
        "      valid_iter_count += 1\n",
        "\n",
        "    total_loss /= iter_count\n",
        "    valid_total_loss /= valid_iter_count\n",
        "    print(\"epoch: {} MSE loss: {:.4f} MSE valid loss: {:.4f}\".format(epoch, total_loss, valid_total_loss))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xrTY4eGMjZy"
      },
      "id": "5xrTY4eGMjZy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ppx31JlmVvu3"
      },
      "id": "Ppx31JlmVvu3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}